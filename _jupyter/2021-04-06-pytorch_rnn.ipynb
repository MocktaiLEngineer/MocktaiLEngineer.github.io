{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we will explore recurrent neural networks (RNNs) and their implementation using PyTorch. While we won't build everything from scratch, we'll still gain valuable insights by implementing certain parts without relying on PyTorch's autograd for gradient computation and backpropagation.\n",
    "\n",
    "I want to emphasize that this post is largely adapted from [this PyTorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html), with modifications to the preprocessing and training steps. I encourage you to check out the original tutorial as supplementary material. Now, let's dive in and get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "The task is to build a simple classification model that can correctly determine the nationality of a person given their name. Put more simply, we want to be able to tell where a particular name is from. \n",
    "\n",
    "## Download\n",
    "\n",
    "We will be using some labeled data from the PyTorch tutorial. We can download it simply by typing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://download.pytorch.org/tutorial/data.zip; unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will download and unzip the files into the current directory, under the folder name of `data`. \n",
    "\n",
    "Now that we have downloaded the data we need, let's take a look at the data in more detail. First, here are the dependencies we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from string import ascii_letters\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from unidecode import unidecode\n",
    "\n",
    "_ = torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first specify a directory, then try to print out all the labels there are. We can then construct a dictionary that maps a language to a numerical label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/names\"\n",
    "\n",
    "lang2label = {\n",
    "    file_name.split(\".\")[0]: torch.tensor([i], dtype=torch.long)\n",
    "    for i, file_name in enumerate(os.listdir(data_dir))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are a total of 18 languages. I wrapped each label as a tensor so that we can use them directly during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Czech': tensor([0]),\n",
       " 'German': tensor([1]),\n",
       " 'Arabic': tensor([2]),\n",
       " 'Japanese': tensor([3]),\n",
       " 'Chinese': tensor([4]),\n",
       " 'Vietnamese': tensor([5]),\n",
       " 'Russian': tensor([6]),\n",
       " 'French': tensor([7]),\n",
       " 'Irish': tensor([8]),\n",
       " 'English': tensor([9]),\n",
       " 'Spanish': tensor([10]),\n",
       " 'Greek': tensor([11]),\n",
       " 'Italian': tensor([12]),\n",
       " 'Portuguese': tensor([13]),\n",
       " 'Scottish': tensor([14]),\n",
       " 'Dutch': tensor([15]),\n",
       " 'Korean': tensor([16]),\n",
       " 'Polish': tensor([17])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the number of languages in some variable so that we can use it later in our model declaration, specifically when we specify the size of the final output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_langs = len(lang2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Now, let's preprocess the names. We first want to use `unidecode` to standardize all names and remove any acute symbols or the likes. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Slusarski'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode(\"Ślusàrski\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a decoded string, we then need to convert it to a tensor so that the model can process it. This can first be done by constructing a `char2idx` mapping, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {letter: i for i, letter in enumerate(ascii_letters + \" .,:;-'\")}\n",
    "num_letters = len(char2idx); num_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are a total of 59 tokens in our character vocabulary. This includes spaces and punctuations, such as ` .,:;-'`. This also means that each name will now be expressed as a tensor of size `(num_char, 59)`; in other words, each character will be a tensor of size `(59,)`. We can now build a function that accomplishes this task, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2tensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, num_letters)\n",
    "    for i, char in enumerate(name):\n",
    "        tensor[i][0][char2idx[char]] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read the code carefully, you'll realize that the output tensor is of size `(num_char, 1, 59)`, which is different from the explanation above. Well, the reason for that extra dimension is that we are using a batch size of 1 in this case. In PyTorch, RNN layers expect the input tensor to be of size `(seq_len, batch_size, input_size)`. Since every name is going to have a different length, we don't batch the inputs for simplicity purposes and simply use each input as a single batch. For a more detailed discussion, check out this [forum discussion](https://discuss.pytorch.org/t/batch-size-position-and-rnn-tutorial/41269/3).\n",
    "\n",
    "Let's quickly verify the output of the `name2tensor()` function with a dummy input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2tensor(\"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "Now we need to build a our dataset with all the preprocessing steps. Let's collect all the decoded and converted tensors in a list, with accompanying labels. The labels can be obtained easily from the file name, for example `german.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = []\n",
    "target_langs = []\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, file)) as f:\n",
    "        lang = file.split(\".\")[0]\n",
    "        names = [unidecode(line.rstrip()) for line in f]\n",
    "        for name in names:\n",
    "            try:\n",
    "                tensor_names.append(name2tensor(name))\n",
    "                target_langs.append(lang2label[lang])\n",
    "            except KeyError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could wrap this in a PyTorch `Dataset` class, but for simplicity sake let's just use a good old `for` loop to feed this data into our model. Since we are dealing with normal lists, we can easily use `sklearn`'s `train_test_split()` to separate the training data from the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    range(len(target_langs)), \n",
    "    test_size=0.1, \n",
    "    shuffle=True, \n",
    "    stratify=target_langs\n",
    ")\n",
    "\n",
    "train_dataset = [\n",
    "    (tensor_names[i], target_langs[i])\n",
    "    for i in train_idx\n",
    "]\n",
    "\n",
    "test_dataset = [\n",
    "    (tensor_names[i], target_langs[i])\n",
    "    for i in test_idx\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many training and testing data we have. Note that we used a `test_size` of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 18063\n",
      "Test: 2007\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building two models: a simple RNN, which is going to be built from scratch, and a GRU-based model using PyTorch's layers.\n",
    "\n",
    "## Simple RNN\n",
    "\n",
    "Now we can build our model. This is a very simple RNN that takes a single character tensor representation as input and produces some prediction and a hidden state, which can be used in the next iteration. Notice that it is just some fully connected layers with a sigmoid non-linearity applied during the hidden state computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        combined = torch.cat((x, hidden_state), 1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call `init_hidden()` at the start of every new batch. For easier training and learning, I decided to use `kaiming_uniform_()` to initialize these hidden states. \n",
    "\n",
    "We can now build our model and start training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MyRNN(num_letters, hidden_size, num_langs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realized that training this model is very unstable, and as you can see the loss jumps up and down quite a bit. Nonetheless, I didn't want to cook my 13-inch MacBook Air so I decided to stop at two epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [3000/18063], Loss: 0.0390\n",
      "Epoch [1/2], Step [6000/18063], Loss: 1.0368\n",
      "Epoch [1/2], Step [9000/18063], Loss: 0.6718\n",
      "Epoch [1/2], Step [12000/18063], Loss: 0.0003\n",
      "Epoch [1/2], Step [15000/18063], Loss: 1.0658\n",
      "Epoch [1/2], Step [18000/18063], Loss: 1.0021\n",
      "Epoch [2/2], Step [3000/18063], Loss: 0.0021\n",
      "Epoch [2/2], Step [6000/18063], Loss: 0.0131\n",
      "Epoch [2/2], Step [9000/18063], Loss: 0.3842\n",
      "Epoch [2/2], Step [12000/18063], Loss: 0.0002\n",
      "Epoch [2/2], Step [15000/18063], Loss: 2.5420\n",
      "Epoch [2/2], Step [18000/18063], Loss: 0.0172\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "print_interval = 3000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_dataset)\n",
    "    for i, (name, label) in enumerate(train_dataset):\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % print_interval == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                f\"Step [{i + 1}/{len(train_dataset)}], \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our model. We could look at other metrics, but accuracy is by far the simplest, so let's go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.2471%\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_samples = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, label in test_dataset:\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == label)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model records a 72 percent accuracy rate. This is very bad, but given how simple the models is and the fact that we only trained the model for two epochs, we can lay back and indulge in momentary happiness knowing that the simple RNN model was at least able to learn something. \n",
    "\n",
    "Let's see how well our model does with some concrete examples. Below is a function that accepts a string as input and outputs a decoded prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2lang = {label.item(): lang for lang, label in lang2label.items()}\n",
    "\n",
    "def myrnn_predict(name):\n",
    "    model.eval()\n",
    "    tensor_name = name2tensor(name)\n",
    "    with torch.no_grad():\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in tensor_name:\n",
    "            output, hidden_state = model(char, hidden_state)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "    model.train()    \n",
    "    return label2lang[pred.item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know if any of these names were actually in the training or testing set; these are just some random names I came up with that I thought would be pretty reasonable. And voila, the results are promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrnn_predict(\"Mike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chinese'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrnn_predict(\"Qin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrnn_predict(\"Slaveya\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to have classified all the names into correct categories! \n",
    "\n",
    "## PyTorch GRU\n",
    "\n",
    "This is cool and all, and I could probably stop here, but I wanted to see how this custom model fares in comparison to, say, a model using PyTorch layers. GRU is probably not fair game for our simple RNN, but let's see how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=num_letters, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_langs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden_state = self.init_hidden()\n",
    "        output, hidden_state = self.gru(x, hidden_state)\n",
    "        output = self.fc(output[-1])\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's declare the model and an optimizer to go with it. Notice that we are using a two-layer GRU, which is already one more than our current RNN implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUModel(num_layers=2, hidden_size=hidden_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [3000/18063], Loss: 1.8497\n",
      "Epoch [1/2], Step [6000/18063], Loss: 0.4908\n",
      "Epoch [1/2], Step [9000/18063], Loss: 1.0299\n",
      "Epoch [1/2], Step [12000/18063], Loss: 0.0855\n",
      "Epoch [1/2], Step [15000/18063], Loss: 0.0053\n",
      "Epoch [1/2], Step [18000/18063], Loss: 2.6417\n",
      "Epoch [2/2], Step [3000/18063], Loss: 0.0004\n",
      "Epoch [2/2], Step [6000/18063], Loss: 0.0008\n",
      "Epoch [2/2], Step [9000/18063], Loss: 0.1446\n",
      "Epoch [2/2], Step [12000/18063], Loss: 0.2125\n",
      "Epoch [2/2], Step [15000/18063], Loss: 3.7883\n",
      "Epoch [2/2], Step [18000/18063], Loss: 0.4862\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_dataset)\n",
    "    for i, (name, label) in enumerate(train_dataset):\n",
    "        output = model(name)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "         \n",
    "        if (i + 1) % print_interval == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                f\"Step [{i + 1}/{len(train_dataset)}], \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training appeared somewhat more stable at first, but we do see a weird jump near the end of the second epoch. This is partially because I didn't use gradient clipping for this GRU model, and we might see better results with clipping applied.\n",
    "\n",
    "Let's see the accuracy of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.4150%\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, label in test_dataset:\n",
    "        output = model(name)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        num_correct += bool(pred == label)\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get an accuracy of around 80 percent for this model. This is better than our simple RNN model, which is somewhat expected given that it had one additional layer and was using a more complicated RNN cell model. \n",
    "\n",
    "Let's see how this model predicts given some raw name string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_predict(name):\n",
    "    model.eval()\n",
    "    tensor_name = name2tensor(name)\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor_name)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "    model.train()\n",
    "    return label2lang[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chinese'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_predict(\"Qin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spanish'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_predict(\"Fernando\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_predict(\"Demirkan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last one is interesting, because it is the name of a close Turkish friend of mine. The model obviously isn't able to tell us that the name is Turkish since it didn't see any data points that were labeled as Turkish, but it tells us what nationality the name might fall under among the 18 labels it has been trained on. It's obviously wrong, but perhaps not too far off in some regards; at least it didn't say Japanese, for instance. It's also not entirely fair game for the model since there are many names that might be described as multi-national: perhaps there is a Russian person with the name of Demirkan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "I learned quite a bit about RNNs by implementing this RNN. It is admittedly simple, and it is somewhat different from the PyTorch layer-based approach in that it requires us to loop through each character manually, but the low-level nature of it forced me to think more about tensor dimensions and the purpose of having a division between the hidden state and output. It was also a healthy reminder of how RNNs can be difficult to train. \n",
    "\n",
    "In the coming posts, we will be looking at sequence-to-sequence models, or seq2seq for short. Ever since I heard about seq2seq, I was fascinated by tthe power of transforming one form of data to another. Although these models cannot be realistically trained on a CPU given the constraints of my local machine, I think implementing them themselves will be an exciting challenge. \n",
    "\n",
    "Catch you up in the next one!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
