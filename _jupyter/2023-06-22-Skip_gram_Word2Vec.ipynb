{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we delve into the skip-gram neural network architecture used in Word2Vec. The purpose of this tutorial is to bypass the typical introductory and abstract explanations about Word2Vec and instead focus on the intricacies of the skip-gram neural network model.\n",
        "\n",
        "## Readings\n",
        "\n",
        "Here are the resources I used to build this notebook. I suggest reading these either beforehand or while you're working on this material.\n",
        "\n",
        "* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of Word2Vec from Chris McCormick\n",
        "* [First Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
        "\n",
        "\n",
        "## The Model\n",
        "The skip-gram neural network model, in its fundamental form, is surprisingly straightforward. However, as we delve into the details, various adjustments and enhancements can complicate the explanation.\n",
        "\n",
        "To begin, let's gain a high-level understanding of our direction. Word2Vec employs a technique commonly utilized in machine learning. We train a simple neural network with a hidden layer to accomplish a specific task. However, we won't actually utilize this neural network for the task it was trained on! Instead, our objective is to grasp the weights of the hidden layer itself, as these weights serve as the \"word vectors\" we aim to learn.\n",
        "\n",
        "## The Fake Task\n",
        "\n",
        "Now, let's delve into the \"fake\" task that we will design the neural network to accomplish. Later on, we will explore how this task indirectly provides us with the desired word vectors.\n",
        "\n",
        "The objective of the neural network is as follows: given a specific word positioned in the middle of a sentence (referred to as the input word), we examine the surrounding words and randomly select one. The network's role is to provide us with the probability of each word in our vocabulary being the chosen \"nearby word.\"\n",
        "\n",
        "```When we mention \"nearby,\" there exists a parameter known as the \"window size\" within the algorithm. Typically, a window size of 5 is used, encompassing 5 preceding words and 5 succeeding words (10 in total).```\n",
        "\n",
        "The output probabilities will indicate the likelihood of finding each vocabulary word in the vicinity of our input word. For instance, if we feed the trained network the input word \"coffee,\" the probabilities will be higher for words like \"mug\" and \"brew\" compared to unrelated words such as \"elephant\" and \"umbrella.\"\n",
        "\n",
        "To train the neural network for this task, we will provide it with word pairs extracted from our training documents. It's okay if you still think this is magic, stick with me till the end and you'll understand how are related words able to cluster together in a high dimensional space.\n",
        "\n"
      ],
      "metadata": {
        "id": "UgG1BxLH-RuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data\n",
        "\n",
        "The below command loads the data for you -\n",
        "\n",
        "1. Downloads the [text8 dataset](http://mattmahoney.net/dc/text8.zip); a file of cleaned up *Wikipedia article text* from Matt Mahoney.\n",
        "2. Unzips the data and places that data in the `data` folder in the home directory.\n",
        "\n",
        "Execute the below command to load the text8 file into your data directory: `data/text8`."
      ],
      "metadata": {
        "id": "7clIt9mmE--V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip && mkdir data && unzip text8.zip -d data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W0nlXHnEl78",
        "outputId": "471a2fa7-08c6-4321-e121-86867b050536"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-22 16:27:13--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 34.198.1.81\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|34.198.1.81|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M  7.63MB/s    in 3.9s    \n",
            "\n",
            "2023-06-22 16:27:18 (7.63 MB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  text8.zip\n",
            "  inflating: data/text8              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's take a look at the data\n",
        "\n"
      ],
      "metadata": {
        "id": "gUEqCYlnSc-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the file, and read its content into 'text'\n",
        "with open('data/text8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgJdy537SrMe",
        "outputId": "fd5b9ebc-6b1d-4bee-ae2f-bffa9998c221"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " anarchism originated as a term of abuse first use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting Word Counts<a name=\"word_counts\"></a>\n",
        "-------------------------\n"
      ],
      "metadata": {
        "id": "fvqDS5yITkp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Convert text into a list of words\n",
        "text_words = text.split()\n",
        "\n",
        "# Use the Counter to count the number of occurences for each word\n",
        "word_counts = Counter(text_words)\n",
        "\n",
        "# Sorting the Counter Dict based on the count values (In descending order)\n",
        "sorted_vocab = sorted(word_counts.items(), key=lambda pair: pair[1], reverse=True)\n",
        "\n",
        "# Convert the dictionary into two numpy arrays so we can do math on it easily.\n",
        "words = np.asarray(list(word_counts.keys()))\n",
        "word_counts = np.asarray(list(word_counts.values()))\n",
        "\n",
        "# Total words in the training set.\n",
        "# Make sure to sum with int64, otherwise it will overflow!\n",
        "total_words = np.sum(word_counts, dtype=np.int64)\n",
        "\n",
        "print('Number of words in vocabulary: {:,}\\n'.format(len(words)))\n",
        "print('Total word occurrences in text8 dataset: {:,}\\n'.format(total_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B28OzBVvWKqn",
        "outputId": "8d23a179-351d-40fe-8627-a09fe6367a3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in vocabulary: 253,854\n",
            "\n",
            "Total word occurrences in text8 dataset: 17,005,207\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just out of curiosity, here are the most frequent and least frequent words."
      ],
      "metadata": {
        "id": "-CS6gGJftvit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The 10 most frequent words:\\n')\n",
        "print('  --Count--    --Word--')\n",
        "\n",
        "# For the first ten word counts...\n",
        "for item in sorted_vocab[:10]:\n",
        "    # Print the count with commas, and pad it to 12 characters.\n",
        "    print('{:>12,}     {:}'.format(item[1], item[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSxys8JZtwof",
        "outputId": "c10bff89-b965-4dae-d5f5-2a0d5c5e72f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 10 most frequent words:\n",
            "\n",
            "  --Count--    --Word--\n",
            "   1,061,396     the\n",
            "     593,677     of\n",
            "     416,629     and\n",
            "     411,764     one\n",
            "     372,201     in\n",
            "     325,873     a\n",
            "     316,376     to\n",
            "     264,975     zero\n",
            "     250,430     nine\n",
            "     192,644     two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The 10 least frequent words:\\n')\n",
        "print('  --Count--    --Word--')\n",
        "\n",
        "# For the first ten word counts...\n",
        "for item in sorted_vocab[:-10:-1]:\n",
        "    # Print the count with commas, and pad it to 12 characters.\n",
        "    print('{:>12,}     {:}'.format(item[1], item[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_TwqyjfxzNx",
        "outputId": "db43127c-a711-4427-8d81-ad7126e4a0e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 10 least frequent words:\n",
            "\n",
            "  --Count--    --Word--\n",
            "           1     exortation\n",
            "           1     fretensis\n",
            "           1     metzuda\n",
            "           1     metzada\n",
            "           1     erniest\n",
            "           1     workmans\n",
            "           1     englander\n",
            "           1     mikhailgorbachev\n",
            "           1     gorbacheva\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing\n",
        "\n",
        "Let us pre-process the text to make it easier for us, and for the model to behave as expected.\n",
        "\n",
        "Here's what I have in mind -\n",
        "\n",
        "1.   Remove less frequent words, to reduce noise in the dataset and to improveme the quality of the word representations.\n",
        "2.   Convert any punctuations into tokens, so for example, a comma is replaced as a \"<COMMA>\" - this will essentially help in other NLP problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "f7qVljHoxpYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def preprocess(text: str) -> list:\n",
        "\n",
        "    # Convert your text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Replace punctuation with tokens so we can use them in our model\n",
        "    text = text.replace(\".\", \" <PERIOD> \")\n",
        "    text = text.replace(\",\", \" <COMMA> \")\n",
        "    text = text.replace('\"', \" <QUOTATION_MARK> \")\n",
        "    text = text.replace(\";\", \" <SEMICOLON> \")\n",
        "    text = text.replace(\"!\", \" <EXCLAMATION_MARK> \")\n",
        "    text = text.replace(\"?\", \" <QUESTION_MARK> \")\n",
        "    text = text.replace(\"(\", \" <LEFT_PAREN> \")\n",
        "    text = text.replace(\")\", \" <RIGHT_PAREN> \")\n",
        "    text = text.replace(\"--\", \" <HYPHENS> \")\n",
        "    text = text.replace(\"?\", \" <QUESTION_MARK> \")\n",
        "    text = text.replace(\":\", \" <COLON> \")\n",
        "\n",
        "    # Split the text into a list of words\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove all words with  5 or fewer occurences\n",
        "    word_counts = Counter(words)\n",
        "    processed_words = [word for word in words if word_counts[word] > 5]\n",
        "\n",
        "    return processed_words\n",
        "\n",
        "words = preprocess(text)\n",
        "\n",
        "print(words[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RydUocjC0S5m",
        "outputId": "5b43e754-b1ce-4124-9cd2-ea12f255e492"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building lookup tables"
      ],
      "metadata": {
        "id": "uE8DRCWxJjI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lookup_tables(words):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary\n",
        "    :param words: Input list of words\n",
        "    :return: Two dictionaries, word_to_int, int_to_word\n",
        "    \"\"\"\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # sorting the words from most to least frequent in text occurrence\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    # create int_to_vocab dictionaries\n",
        "    int_to_word = {i: word for i, word in enumerate(sorted_vocab)}\n",
        "    word_to_int = {word: i for i, word in int_to_word.items()}\n",
        "\n",
        "    return word_to_int, int_to_word\n",
        "\n",
        "word_to_int, int_to_word = create_lookup_tables(words)\n",
        "\n",
        "encoded_words = [word_to_int[word] for word in words]\n",
        "\n",
        "print(encoded_words[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXcHni0NVHyQ",
        "outputId": "f7ec147c-6851-48d2-96c8-697ebb8c565d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('  --Word--    --Int--')\n",
        "\n",
        "# Iterate over the items of the word_to_int dictionary\n",
        "for word, id in list(word_to_int.items())[:10]:\n",
        "    print(f'  {word:<10} {id}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyJJfIB1J92g",
        "outputId": "2d914664-6287-460a-fd0b-267afe07624d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  --Word--    --Int--\n",
            "  the        0\n",
            "  of         1\n",
            "  and        2\n",
            "  one        3\n",
            "  in         4\n",
            "  a          5\n",
            "  to         6\n",
            "  zero       7\n",
            "  nine       8\n",
            "  two        9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('  --Int--    --Word--')\n",
        "\n",
        "# Iterate over the items of the int_to_word dictionary\n",
        "for id, word in list(int_to_word.items())[:10]:\n",
        "    print(f'  {id:<10} {word}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSEXn6pP30Ug",
        "outputId": "40689d0f-eef5-48fe-d1eb-72ab19f3660a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  --Int--    --Word--\n",
            "  0          the\n",
            "  1          of\n",
            "  2          and\n",
            "  3          one\n",
            "  4          in\n",
            "  5          a\n",
            "  6          to\n",
            "  7          zero\n",
            "  8          nine\n",
            "  9          two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subsampling\n",
        "\n",
        "Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we'll discard it with probability given by\n",
        "\n",
        "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
        "\n",
        "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.\n",
        "\n",
        "$$ P(0) = 1 - \\sqrt{\\frac{1*10^{-5}}{1*10^6/16*10^6}} = 0.98735 $$"
      ],
      "metadata": {
        "id": "pN73wMh3Rblw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set the threshold for subsampling\n",
        "threshold = 1e-5\n",
        "\n",
        "# Count the occurrences of each word in the encoded_words list\n",
        "word_counts = Counter(encoded_words)\n",
        "\n",
        "# Calculate the total count of words in the encoded_words list\n",
        "total_count = len(encoded_words)\n",
        "\n",
        "# Calculate the frequencies of each word\n",
        "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
        "\n",
        "# Calculate the probability of dropping each word based on its frequency\n",
        "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
        "\n",
        "# Discard some frequent words based on the subsampling equation\n",
        "# Create a new list of words for training, keeping only the words that were not dropped\n",
        "train_words = [word for word in encoded_words if random.random() < (1 - p_drop[word])]\n",
        "\n",
        "# Print the first 30 words in the train_words list\n",
        "print(train_words[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRp0s5NFRmSS",
        "outputId": "a5a92996-f781-403e-8f9f-1a18e71b96e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5233, 194, 45, 58, 10571, 27349, 15067, 58112, 194, 190, 58, 10712, 1324, 708, 7088, 1052, 320, 44611, 2877, 5233, 1134, 2621, 8983, 279, 4147, 59, 6437, 5233, 1137, 4860]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making batches\n",
        "\n",
        "Now that our data is in good shape, we need to get it into the proper form to pass it into our network. With the skip-gram architecture, for each word in the text, we want to define a surrounding _context_ and grab all the words in a window around that word, with size $C$.\n",
        "\n",
        "From [Mikolov et al.](https://arxiv.org/pdf/1301.3781.pdf):\n",
        "\n",
        "\"Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples... If we choose $C = 5$, for each training word we will select randomly a number $R$ in range $[ 1: C ]$, and then use $R$ words from history and $R$ words from the future of the current word as correct labels.\"\n",
        "\n",
        "> **Exercise:** Implement a function `get_target` that receives a list of words, an index, and a window size, then returns a list of words in the window around the index. Make sure to use the algorithm described above, where you chose a random number of words to from the window.\n",
        "\n",
        "Say, we have an input and we're interested in the idx=2 token, `741`:\n",
        "```\n",
        "[5233, 58, 741, 10571, 27349, 0, 15067, 58112, 3580, 58, 10712]\n",
        "```\n",
        "\n",
        "For `R=2`, `get_target` should return a list of four values:\n",
        "```\n",
        "[5233, 58, 10571, 27349]\n",
        "```"
      ],
      "metadata": {
        "id": "grY_KfmlUTAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target(words, idx, window_size=5):\n",
        "    ''' Get a list of words in a window around an index. '''\n",
        "\n",
        "    R = np.random.randint(1, window_size+1)\n",
        "    start = idx - R if (idx - R) > 0 else 0\n",
        "    stop = idx + R\n",
        "    target_words = words[start:idx] + words[idx+1:stop+1]\n",
        "\n",
        "    return list(target_words)"
      ],
      "metadata": {
        "id": "_6_zRRn6WEXy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell multiple times to check for random window selection\n",
        "int_text = [i for i in range(10)]\n",
        "print('Input: ', int_text)\n",
        "\n",
        "idx=5 # word index of interest\n",
        "\n",
        "target = get_target(int_text, idx=idx, window_size=5)\n",
        "print('Target: ', target)  # you should get some indices around the idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uraBeA_LWIYJ",
        "outputId": "30d526af-f87a-4840-9119-6d1b24a3ec02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Target:  [2, 3, 4, 6, 7, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Batches\n",
        "\n",
        "Here's a generator function that returns batches of input and target data for our model, using the `get_target` function from above. The idea is that it grabs `batch_size` words from a words list. Then for each of those batches, it gets the target words in a window."
      ],
      "metadata": {
        "id": "-m_dka15WKyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(words, batch_size, window_size=5):\n",
        "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
        "\n",
        "    n_batches = len(words)//batch_size\n",
        "\n",
        "    # only full batches\n",
        "    words = words[:n_batches*batch_size]\n",
        "\n",
        "    for idx in range(0, len(words), batch_size):\n",
        "        x, y = [], []\n",
        "        batch = words[idx:idx+batch_size]\n",
        "        for ii in range(len(batch)):\n",
        "            batch_x = batch[ii]\n",
        "            batch_y = get_target(batch, ii, window_size)\n",
        "            y.extend(batch_y)\n",
        "            x.extend([batch_x]*len(batch_y))\n",
        "        yield x, y\n"
      ],
      "metadata": {
        "id": "HZG_h8qEWNYg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Validation\n",
        "\n",
        "Here, I'm creating a function that will help us observe our model as it learns. We're going to choose a few common words and few uncommon words. Then, we'll print out the closest words to them using the cosine similarity:\n",
        "\n",
        "$$\n",
        "\\mathrm{similarity} = \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}\n",
        "$$\n",
        "\n",
        "\n",
        "We can encode the validation words as vectors $\\vec{a}$ using the embedding table, then calculate the similarity with each word vector $\\vec{b}$ in the embedding table. With the similarities, we can print out the validation words and words in our embedding table semantically similar to those words. It's a nice way to check that our embedding table is grouping together words with similar semantic meanings."
      ],
      "metadata": {
        "id": "DD8JZImbs0FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(embedding, valid_size=8, valid_window=100, device='cpu'):\n",
        "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
        "        Here, embedding should be a PyTorch embedding module.\n",
        "    \"\"\"\n",
        "\n",
        "    # Here we're calculating the cosine similarity between some random words and\n",
        "    # our embedding vectors. With the similarities, we can look at what words are\n",
        "    # close to our random words.\n",
        "\n",
        "    # sim = (a . b) / |a||b|\n",
        "\n",
        "    embed_vectors = embedding.weight\n",
        "\n",
        "    # magnitude of embedding vectors, |b|\n",
        "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
        "\n",
        "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent\n",
        "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
        "    valid_examples = np.append(valid_examples,\n",
        "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
        "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
        "\n",
        "    valid_vectors = embedding(valid_examples)\n",
        "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
        "\n",
        "    return valid_examples, similarities"
      ],
      "metadata": {
        "id": "WxaWYrZwsv3Y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SkipGram model\n",
        "\n",
        "Define and train the SkipGram model.\n",
        "> You'll need to define an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) and a final, softmax output layer.\n",
        "\n",
        "An Embedding layer takes in a number of inputs, importantly:\n",
        "* **num_embeddings** – the size of the dictionary of embeddings, or how many rows you'll want in the embedding weight matrix\n",
        "* **embedding_dim** – the size of each embedding vector; the embedding dimension"
      ],
      "metadata": {
        "id": "3qGP-E4-s5Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, n_vocab, n_embed):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
        "        self.output = nn.Linear(n_embed, n_vocab)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        scores = self.output(x)\n",
        "        log_ps = self.log_softmax(scores)\n",
        "\n",
        "        return log_ps"
      ],
      "metadata": {
        "id": "msfNl4wqs7si"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Below is our training loop, and I recommend that you train on GPU, if available.\n",
        "\n",
        "**Note that, because we applied a softmax function to our model output, we are using NLLLoss** as opposed to cross entropy. This is because Softmax  in combination with NLLLoss = CrossEntropy loss ."
      ],
      "metadata": {
        "id": "p_Su7ArWs-bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "embedding_dim=300 # you can change, if you want\n",
        "\n",
        "model = SkipGram(len(word_to_int), embedding_dim).to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "print_every = 2500\n",
        "steps = 0\n",
        "epochs = 5\n",
        "\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "\n",
        "    # get input and target batches\n",
        "    for inputs, targets in get_batches(train_words, 512):\n",
        "        steps += 1\n",
        "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        log_ps = model(inputs)\n",
        "        loss = criterion(log_ps, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if steps % print_every == 0:\n",
        "            # getting examples and similarities\n",
        "            valid_examples, valid_similarities = cosine_similarity(model.embed, device=device)\n",
        "            _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n",
        "\n",
        "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
        "            for i, valid_idx in enumerate(valid_examples):\n",
        "                closest_words = [int_to_word[idx.item()] for idx in closest_idxs[i]][1:]\n",
        "                print(int_to_word[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
        "            print(\"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PGn0q9WtCUZ",
        "outputId": "aba55b28-1f92-4cf0-f0c7-a56678c7d1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "up | nablus, amelia, enchantments, civilize, adding\n",
            "during | poinsot, jannah, shambhala, deva, bezprym\n",
            "such | portrayed, megawatts, westminsters, to, ayers\n",
            "about | innings, frigate, persist, fide, alternation\n",
            "consists | hires, wheel, rida, sesame, exotics\n",
            "ice | kaiserliche, melatonin, reed, domine, psych\n",
            "frac | homicidal, filmi, emrys, wilbert, byelorussian\n",
            "police | billiard, lavinia, zagros, possums, tab\n",
            "...\n",
            "seven | nine, poorest, five, claudia, collectivisation\n",
            "often | musial, entails, orchha, confusing, zimbardo\n",
            "eight | leitch, aho, krishnan, redesigned, zero\n",
            "by | micros, critica, balk, healthier, pre\n",
            "recorded | paintings, mano, thrown, decks, iib\n",
            "freedom | vorbis, marginalize, exoteric, earn, blair\n",
            "mainly | verso, singapore, annex, buonarroti, hikari\n",
            "liberal | sturmgewehr, hugging, lunas, openly, jng\n",
            "...\n",
            "b | antonio, mathematician, tat, nicky, shanghainese\n",
            "their | residence, lorne, liking, tgs, until\n",
            "its | stalled, nashwaak, socony, chabad, of\n",
            "than | triumphed, accurate, unstoppable, biopolymers, pressurized\n",
            "discovered | nighttime, voracious, ascendant, herakles, pi\n",
            "numerous | bribery, eddic, theobromine, mongo, cautiously\n",
            "pressure | danforth, extraction, masjid, kara, essequibo\n",
            "woman | wasn, roses, milah, lipstick, exports\n",
            "...\n",
            "had | he, barely, railroaders, sale, addressed\n",
            "who | erudition, prevailed, aeschylus, pascual, elderly\n",
            "may | observant, duplication, individualist, acumen, recited\n",
            "they | stalk, unborn, protects, forceful, even\n",
            "discovered | pi, nighttime, endowed, voracious, authorship\n",
            "marriage | jondo, ultimate, attitudes, doves, caesarian\n",
            "applied | student, prism, stargate, papp, dai\n",
            "dr | gill, backpacker, socketed, mirroring, jun\n",
            "...\n",
            "who | receive, their, erudition, humbert, adultery\n",
            "may | duplication, observant, not, myriad, provisions\n",
            "up | anything, to, before, voltages, your\n",
            "the | a, of, in, for, which\n",
            "brother | taisho, unison, isolates, odysseus, died\n",
            "institute | engineering, textbook, vayikra, cookbook, kangaroo\n",
            "joseph | insistence, martin, kohl, freethought, stacey\n",
            "centre | equal, erected, rancid, melchett, watt\n",
            "...\n",
            "state | act, federal, states, guthrum, exercised\n",
            "that | the, be, this, to, himself\n",
            "no | illusions, feminine, xa, catechetical, replacement\n",
            "their | they, them, are, have, to\n",
            "rise | empire, pico, declan, anaxagoras, became\n",
            "experience | camphor, talkative, artemis, pantheists, akin\n",
            "arts | disciplines, art, education, intv, timelines\n",
            "something | anything, hopefully, hiker, furry, silly\n",
            "...\n",
            "for | also, a, ml, more, and\n",
            "use | used, useful, vendors, standard, or\n",
            "see | history, article, list, of, links\n",
            "were | many, their, was, by, themselves\n",
            "orthodox | ecumenical, christian, churches, church, conservative\n",
            "pre | ingrained, expansion, handsets, mythologies, raid\n",
            "question | questions, answer, assumptions, what, prescribes\n",
            "road | amtrak, lanes, roads, highway, autopia\n",
            "...\n",
            "it | is, be, to, come, but\n",
            "state | act, federal, legislature, government, missouri\n",
            "of | in, the, and, as, by\n",
            "also | other, with, as, are, for\n",
            "orthodox | church, ecumenical, christian, conservative, churches\n",
            "proposed | freie, subcommittee, kuiper, jlp, principles\n",
            "operating | hardware, user, unix, lotus, system\n",
            "resources | web, soils, arable, land, private\n",
            "...\n",
            "as | is, or, used, the, of\n",
            "four | six, five, three, two, one\n",
            "i | we, t, me, you, my\n",
            "other | such, also, various, used, refer\n",
            "hit | hits, albums, pop, album, billboard\n",
            "woman | her, women, birth, male, husband\n",
            "question | questions, answer, whether, answers, answered\n",
            "engineering | technology, institute, disciplines, development, mathematics\n",
            "...\n",
            "use | used, using, are, available, commonly\n",
            "called | is, which, referred, then, usually\n",
            "may | not, or, remaining, duplication, consent\n",
            "first | in, the, eight, of, two\n",
            "operating | unix, platforms, platform, interface, hardware\n",
            "proposed | suggested, agreed, agreement, freie, xlii\n",
            "additional | each, provision, fewer, esti, browser\n",
            "troops | forces, battle, war, army, allied\n",
            "...\n"
          ]
        }
      ]
    }
  ]
}