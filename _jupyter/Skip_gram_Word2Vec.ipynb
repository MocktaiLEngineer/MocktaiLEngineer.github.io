{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UgG1BxLH-RuW"
      },
      "source": [
        "# Skip-gram Word2Vec\n",
        "\n",
        "In this tutorial, we delve into the skip-gram neural network architecture used in Word2Vec. The purpose of this tutorial is to bypass the typical introductory and abstract explanations about Word2Vec and instead focus on the intricacies of the skip-gram neural network model.\n",
        "\n",
        "## Readings\n",
        "\n",
        "Here are the resources I used to build this notebook. I suggest reading these either beforehand or while you're working on this material.\n",
        "\n",
        "* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of Word2Vec from Chris McCormick\n",
        "* [First Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
        "\n",
        "\n",
        "## The Model\n",
        "The skip-gram neural network model, in its fundamental form, is surprisingly straightforward. However, as we delve into the details, various adjustments and enhancements can complicate the explanation.\n",
        "\n",
        "To begin, let's gain a high-level understanding of our direction. Word2Vec employs a technique commonly utilized in machine learning. We train a simple neural network with a hidden layer to accomplish a specific task. However, we won't actually utilize this neural network for the task it was trained on! Instead, our objective is to grasp the weights of the hidden layer itself, as these weights serve as the \"word vectors\" we aim to learn.\n",
        "\n",
        "## The Fake Task\n",
        "\n",
        "Now, let's delve into the \"fake\" task that we will design the neural network to accomplish. Later on, we will explore how this task indirectly provides us with the desired word vectors.\n",
        "\n",
        "The objective of the neural network is as follows: given a specific word positioned in the middle of a sentence (referred to as the input word), we examine the surrounding words and randomly select one. The network's role is to provide us with the probability of each word in our vocabulary being the chosen \"nearby word.\"\n",
        "\n",
        "```When we mention \"nearby,\" there exists a parameter known as the \"window size\" within the algorithm. Typically, a window size of 5 is used, encompassing 5 preceding words and 5 succeeding words (10 in total).```\n",
        "\n",
        "The output probabilities will indicate the likelihood of finding each vocabulary word in the vicinity of our input word. For instance, if we feed the trained network the input word \"coffee,\" the probabilities will be higher for words like \"mug\" and \"brew\" compared to unrelated words such as \"elephant\" and \"umbrella.\"\n",
        "\n",
        "To train the neural network for this task, we will provide it with word pairs extracted from our training documents. It's okay if you still think this is magic, stick with me till the end and you'll understand how are related words able to cluster together in a high dimensional space.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7clIt9mmE--V"
      },
      "source": [
        "## Loading Data\n",
        "\n",
        "The below command loads the data for you -\n",
        "\n",
        "1. Downloads the [text8 dataset](http://mattmahoney.net/dc/text8.zip); a file of cleaned up *Wikipedia article text* from Matt Mahoney.\n",
        "2. Unzips the data and places that data in the `data` folder in the home directory.\n",
        "\n",
        "Execute the below command to load the text8 file into your data directory: `data/text8`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W0nlXHnEl78",
        "outputId": "91bccfb0-40a7-4bea-dc2d-571288a685f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-21 06:28:39--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 34.198.1.81\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|34.198.1.81|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-06-21 06:28:39 (216 MB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  text8.zip\n",
            "  inflating: data/text8              \n"
          ]
        }
      ],
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip && mkdir data && unzip text8.zip -d data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gUEqCYlnSc-O"
      },
      "source": [
        "# Let's take a look at the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgJdy537SrMe",
        "outputId": "4cc4d13b-3cf7-42bb-fa47-37725ab89783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " anarchism originated as a term of abuse first use\n"
          ]
        }
      ],
      "source": [
        "# Open the file, and read its content into 'text'\n",
        "with open('data/text8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:50])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fvqDS5yITkp8"
      },
      "source": [
        "## Inspecting Word Counts<a name=\"word_counts\"></a>\n",
        "-------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "B28OzBVvWKqn"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Convert text into a list of words\n",
        "text_words = text.split()\n",
        "\n",
        "# Use the Counter to count the number of occurences for each word\n",
        "word_counts = Counter(text_words)\n",
        "\n",
        "# Sorting the Counter Dict based on the count values (In descending order)\n",
        "sorted_vocab = sorted(word_counts.items(), key=lambda pair: pair[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3QVB3oSWp66",
        "outputId": "5141bf76-3855-4351-e3a6-d5da6d567b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in vocabulary: 253,854\n",
            "\n",
            "Total word occurrences in Wikipedia: 17,005,207\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert the dictionary into two numpy arrays so we can do math on it easily.\n",
        "words = np.asarray(list(word_counts.keys()))\n",
        "word_counts = np.asarray(list(word_counts.values()))\n",
        "\n",
        "# Total words in the training set.\n",
        "# Make sure to sum with int64, otherwise it will overflow!\n",
        "total_words = np.sum(word_counts, dtype=np.int64)\n",
        "\n",
        "print('Number of words in vocabulary: {:,}\\n'.format(len(words)))\n",
        "print('Total word occurrences in Wikipedia: {:,}\\n'.format(total_words))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-CS6gGJftvit"
      },
      "source": [
        "Just out of curiosity, here are the most frequent and least frequent words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSxys8JZtwof",
        "outputId": "fd6ae7c0-e59f-445b-9d13-2ba48a81a9ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 10 most frequest words:\n",
            "\n",
            "  --Count--    --Word--\n",
            "   1,061,396     the\n",
            "     593,677     of\n",
            "     416,629     and\n",
            "     411,764     one\n",
            "     372,201     in\n",
            "     325,873     a\n",
            "     316,376     to\n",
            "     264,975     zero\n",
            "     250,430     nine\n",
            "     192,644     two\n"
          ]
        }
      ],
      "source": [
        "print('The 10 most frequent words:\\n')\n",
        "print('  --Count--    --Word--')\n",
        "\n",
        "# For the first ten word counts...\n",
        "for item in sorted_vocab[:10]:\n",
        "    # Print the count with commas, and pad it to 12 characters.\n",
        "    print('{:>12,}     {:}'.format(item[1], item[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_TwqyjfxzNx",
        "outputId": "e91eb02c-a607-4391-8e44-43783bc714a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 10 least frequent words:\n",
            "\n",
            "  --Count--    --Word--\n",
            "           1     exortation\n",
            "           1     fretensis\n",
            "           1     metzuda\n",
            "           1     metzada\n",
            "           1     erniest\n",
            "           1     workmans\n",
            "           1     englander\n",
            "           1     mikhailgorbachev\n",
            "           1     gorbacheva\n"
          ]
        }
      ],
      "source": [
        "print('The 10 least frequent words:\\n')\n",
        "print('  --Count--    --Word--')\n",
        "\n",
        "# For the first ten word counts...\n",
        "for item in sorted_vocab[:-10:-1]:\n",
        "    # Print the count with commas, and pad it to 12 characters.\n",
        "    print('{:>12,}     {:}'.format(item[1], item[0]))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
